{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using custom containers with Vertex AI Training\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Learn how to create a train and a validation split with BigQuery\n",
    "1. Learn how to wrap a machine learning model into a Docker container and train in on Vertex AI\n",
    "1. Learn how to use the hyperparameter tuning engine on Vertex AI to find the best hyperparameters\n",
    "1. Learn how to deploy a trained machine learning model on Vertex AI as a REST API and query it\n",
    "\n",
    "In this lab, you develop, package as a docker image, and run on **Vertex AI Training** a training application that trains a multi-class classification model that predicts the type of forest cover from cartographic data. The [dataset](../../../datasets/covertype/README.md) used in the lab is based on **Covertype Data Set** from UCI Machine Learning Repository.\n",
    "\n",
    "The training code uses `scikit-learn` for data pre-processing and modeling. The code has been instrumented using the `hypertune` package so it can be used with **Vertex AI** hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update   `REGION`, and `ARTIFACT_STORE`  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for Vertex AI Training and Prediction\n",
    "- `ARTIFACT_STORE` - A GCS bucket in the created in the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_DIR_ROOT\"] = JOB_DIR_ROOT\n",
    "os.environ[\"TRAINING_FILE_PATH\"] = TRAINING_FILE_PATH\n",
    "os.environ[\"VALIDATION_FILE_PATH\"] = VALIDATION_FILE_PATH\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the `ARTIFACT_STORE` bucket if it's not there. Note that this bucket should be created in the region specified in the variable `REGION` (if you have already a bucket with this name in a different region than `REGION`, you may want to change the `ARTIFACT_STORE` name so that you can recreate a bucket in `REGION` with the command in the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-04-28475851fd68-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'qwiklabs-gcp-04-28475851fd68:covertype_dataset' successfully created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r13ce0d3a1bff6465_0000017fd6922900_1 ... (2s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://workshop-datasets/covertype/small/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Covertype dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 577.65query/s]                          \n",
      "Downloading: 100%|██████████| 15/15 [00:01<00:00, 12.65rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area</th>\n",
       "      <th>Soil_Type</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2067</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>270</td>\n",
       "      <td>9</td>\n",
       "      <td>755</td>\n",
       "      <td>184</td>\n",
       "      <td>196</td>\n",
       "      <td>145</td>\n",
       "      <td>900</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2702</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2574</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>319</td>\n",
       "      <td>20</td>\n",
       "      <td>1419</td>\n",
       "      <td>216</td>\n",
       "      <td>235</td>\n",
       "      <td>156</td>\n",
       "      <td>1595</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2559</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>16</td>\n",
       "      <td>1113</td>\n",
       "      <td>218</td>\n",
       "      <td>238</td>\n",
       "      <td>156</td>\n",
       "      <td>1332</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2647</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>402</td>\n",
       "      <td>94</td>\n",
       "      <td>641</td>\n",
       "      <td>212</td>\n",
       "      <td>229</td>\n",
       "      <td>155</td>\n",
       "      <td>1104</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2651</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>335</td>\n",
       "      <td>103</td>\n",
       "      <td>488</td>\n",
       "      <td>215</td>\n",
       "      <td>233</td>\n",
       "      <td>156</td>\n",
       "      <td>1381</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2647</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>417</td>\n",
       "      <td>94</td>\n",
       "      <td>648</td>\n",
       "      <td>212</td>\n",
       "      <td>229</td>\n",
       "      <td>155</td>\n",
       "      <td>1082</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2639</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>366</td>\n",
       "      <td>80</td>\n",
       "      <td>589</td>\n",
       "      <td>206</td>\n",
       "      <td>222</td>\n",
       "      <td>154</td>\n",
       "      <td>1041</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2590</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>201</td>\n",
       "      <td>13</td>\n",
       "      <td>1200</td>\n",
       "      <td>216</td>\n",
       "      <td>235</td>\n",
       "      <td>156</td>\n",
       "      <td>1719</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2447</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>631</td>\n",
       "      <td>213</td>\n",
       "      <td>232</td>\n",
       "      <td>156</td>\n",
       "      <td>711</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2705</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2501</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>228</td>\n",
       "      <td>31</td>\n",
       "      <td>1012</td>\n",
       "      <td>211</td>\n",
       "      <td>228</td>\n",
       "      <td>155</td>\n",
       "      <td>930</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>1746</td>\n",
       "      <td>213</td>\n",
       "      <td>232</td>\n",
       "      <td>156</td>\n",
       "      <td>886</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2705</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2641</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>15</td>\n",
       "      <td>1518</td>\n",
       "      <td>217</td>\n",
       "      <td>236</td>\n",
       "      <td>156</td>\n",
       "      <td>182</td>\n",
       "      <td>Commanche</td>\n",
       "      <td>C2705</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1315</td>\n",
       "      <td>217</td>\n",
       "      <td>236</td>\n",
       "      <td>156</td>\n",
       "      <td>551</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2717</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2511</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>212</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>200</td>\n",
       "      <td>215</td>\n",
       "      <td>152</td>\n",
       "      <td>524</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2266</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>162</td>\n",
       "      <td>100</td>\n",
       "      <td>1045</td>\n",
       "      <td>175</td>\n",
       "      <td>185</td>\n",
       "      <td>140</td>\n",
       "      <td>738</td>\n",
       "      <td>Cache</td>\n",
       "      <td>C2717</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0        2067       0     21                               270   \n",
       "1        2574       0      2                               319   \n",
       "2        2559       0      0                               510   \n",
       "3        2647       0      6                               402   \n",
       "4        2651       0      3                               335   \n",
       "5        2647       0      6                               417   \n",
       "6        2639       0     10                               366   \n",
       "7        2590       0      2                               201   \n",
       "8        2447       0      4                                 0   \n",
       "9        2501       0      6                               228   \n",
       "10       2500       0      4                                30   \n",
       "11       2641       0      1                                90   \n",
       "12       2346       0      1                                 0   \n",
       "13       2511       0     13                               212   \n",
       "14       2266       0     25                               162   \n",
       "\n",
       "    Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                9                              755   \n",
       "1                               20                             1419   \n",
       "2                               16                             1113   \n",
       "3                               94                              641   \n",
       "4                              103                              488   \n",
       "5                               94                              648   \n",
       "6                               80                              589   \n",
       "7                               13                             1200   \n",
       "8                                0                              631   \n",
       "9                               31                             1012   \n",
       "10                               3                             1746   \n",
       "11                              15                             1518   \n",
       "12                               0                             1315   \n",
       "13                              42                               42   \n",
       "14                             100                             1045   \n",
       "\n",
       "    Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0             184             196            145   \n",
       "1             216             235            156   \n",
       "2             218             238            156   \n",
       "3             212             229            155   \n",
       "4             215             233            156   \n",
       "5             212             229            155   \n",
       "6             206             222            154   \n",
       "7             216             235            156   \n",
       "8             213             232            156   \n",
       "9             211             228            155   \n",
       "10            213             232            156   \n",
       "11            217             236            156   \n",
       "12            217             236            156   \n",
       "13            200             215            152   \n",
       "14            175             185            140   \n",
       "\n",
       "    Horizontal_Distance_To_Fire_Points Wilderness_Area Soil_Type  Cover_Type  \n",
       "0                                  900           Cache     C2702           5  \n",
       "1                                 1595       Commanche     C2703           4  \n",
       "2                                 1332       Commanche     C2703           2  \n",
       "3                                 1104       Commanche     C2703           2  \n",
       "4                                 1381       Commanche     C2703           2  \n",
       "5                                 1082       Commanche     C2703           2  \n",
       "6                                 1041       Commanche     C2703           2  \n",
       "7                                 1719       Commanche     C2703           1  \n",
       "8                                  711       Commanche     C2705           5  \n",
       "9                                  930       Commanche     C2705           1  \n",
       "10                                 886       Commanche     C2705           5  \n",
       "11                                 182       Commanche     C2705           2  \n",
       "12                                 551           Cache     C2717           3  \n",
       "13                                 524           Cache     C2717           1  \n",
       "14                                 738           Cache     C2717           2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM `covertype_dataset.covertype`\n",
    "\n",
    "LIMIT 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation splits\n",
    "\n",
    "Use BigQuery to sample training and validation splits and save them to GCS storage\n",
    "### Create a training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r6448157294ad4306_0000017fd69517de_1 ... (2s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.training \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) IN (1, 2, 3, 4)' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-04-28475851fd68-kfp-artifact-store/data/training/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "!echo $TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r75a4cc7b9ddc3b59_0000017fd69620d4_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.training \\\n",
    "$TRAINING_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r238e1b908521e45e_0000017fd697619b_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq query \\\n",
    "-n 0 \\\n",
    "--destination_table covertype_dataset.validation \\\n",
    "--replace \\\n",
    "--use_legacy_sql=false \\\n",
    "'SELECT * \\\n",
    "FROM `covertype_dataset.covertype` AS cover \\\n",
    "WHERE \\\n",
    "MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), 10) = 5' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r5fd42089bdc43753_0000017fd6977cfa_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract \\\n",
    "--destination_format CSV \\\n",
    "covertype_dataset.validation \\\n",
    "$VALIDATION_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40009, 13)\n",
      "(10027, 13)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_FILE_PATH)\n",
    "df_validation = pd.read_csv(VALIDATION_FILE_PATH)\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a training application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the `sklearn` training pipeline.\n",
    "\n",
    "The training pipeline preprocesses data by standardizing all numeric features using `sklearn.preprocessing.StandardScaler` and encoding all categorical features using `sklearn.preprocessing.OneHotEncoder`. It uses stochastic gradient descent linear classifier (`SGDClassifier`) for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_indexes = slice(0, 10)\n",
    "categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_feature_indexes),\n",
    "        (\"cat\", OneHotEncoder(), categorical_feature_indexes),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", SGDClassifier(loss=\"log\", tol=1e-3)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all numeric features to `float64`\n",
    "\n",
    "To avoid warning messages from `StandardScaler` all numeric features are converted to `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_type_map = {\n",
    "    feature: \"float64\" for feature in df_train.columns[numeric_feature_indexes]\n",
    "}\n",
    "\n",
    "df_train = df_train.astype(num_features_type_map)\n",
    "df_validation = df_validation.astype(num_features_type_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                  slice(0, 10, None)),\n",
       "                                                 ('cat', OneHotEncoder(),\n",
       "                                                  slice(10, 12, None))])),\n",
       "                ('classifier',\n",
       "                 SGDClassifier(alpha=0.001, loss='log', max_iter=200))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train.drop(\"Cover_Type\", axis=1)\n",
    "y_train = df_train[\"Cover_Type\"]\n",
    "X_validation = df_validation.drop(\"Cover_Type\", axis=1)\n",
    "y_validation = df_validation[\"Cover_Type\"]\n",
    "\n",
    "pipeline.set_params(classifier__alpha=0.001, classifier__max_iter=200)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the trained model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6926298992719657\n"
     ]
    }
   ],
   "source": [
    "accuracy = pipeline.score(X_validation, y_validation)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the hyperparameter tuning application.\n",
    "Since the training run on this dataset is computationally expensive you can benefit from running a distributed hyperparameter tuning job on Vertex AI Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = \"training_app\"\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the tuning script. \n",
    "\n",
    "Notice the use of the `hypertune` package to report the `accuracy` optimization metric to Vertex AI hyperparameter tuning service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_app/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/train.py\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_feature_indexes),\n",
    "        ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log',tol=1e-3))\n",
    "    ])\n",
    "\n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "        y_validation = df_validation['Cover_Type']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "          hyperparameter_metric_tag='accuracy',\n",
    "          metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "Notice that we are installing specific versions of `scikit-learn` and `pandas` in the training image. This is done to make sure that the training runtime in the training container is aligned with the serving runtime in the serving container. \n",
    "\n",
    "Make sure to update the URI for the base image so that it points to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the Dockerfile below so that it copies the 'train.py' file into the container\n",
    "at `/app` and runs it when the container is started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY ./train.py /app/\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the docker image. \n",
    "\n",
    "You use **Cloud Build** to build the image and push it your project's **Container Registry**. As you use the remote cloud service to build the image, you don't need a local installation of Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.6 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://qwiklabs-gcp-04-28475851fd68_cloudbuild/source/1648642926.912855-fda4dbd88f4643f0877ed1e0991aa950.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-04-28475851fd68/locations/global/builds/c024643e-23fd-4a81-acff-47e3b4f75f17].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/c024643e-23fd-4a81-acff-47e3b4f75f17?project=1081382100180].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c024643e-23fd-4a81-acff-47e3b4f75f17\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-04-28475851fd68_cloudbuild/source/1648642926.912855-fda4dbd88f4643f0877ed1e0991aa950.tgz#1648642927158948\n",
      "Copying gs://qwiklabs-gcp-04-28475851fd68_cloudbuild/source/1648642926.912855-fda4dbd88f4643f0877ed1e0991aa950.tgz#1648642927158948...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "7c3b88808835: Already exists\n",
      "382fcf64a9ea: Pulling fs layer\n",
      "d764c2aa40d3: Pulling fs layer\n",
      "90cc2e264020: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "395e65f0ab42: Pulling fs layer\n",
      "9e19ad4dbd7d: Pulling fs layer\n",
      "957c609522d8: Pulling fs layer\n",
      "6a5e2168e631: Pulling fs layer\n",
      "5740bb01bc78: Pulling fs layer\n",
      "be09da654f5c: Pulling fs layer\n",
      "288d40a4f176: Pulling fs layer\n",
      "e2d3eec75c0c: Pulling fs layer\n",
      "3769728eb7d7: Pulling fs layer\n",
      "211e30f752a4: Pulling fs layer\n",
      "ae6a5f7af5b1: Pulling fs layer\n",
      "274bb2dca45b: Pulling fs layer\n",
      "4105864a46df: Pulling fs layer\n",
      "9e19ad4dbd7d: Waiting\n",
      "957c609522d8: Waiting\n",
      "6a5e2168e631: Waiting\n",
      "5740bb01bc78: Waiting\n",
      "be09da654f5c: Waiting\n",
      "288d40a4f176: Waiting\n",
      "e2d3eec75c0c: Waiting\n",
      "3769728eb7d7: Waiting\n",
      "211e30f752a4: Waiting\n",
      "ae6a5f7af5b1: Waiting\n",
      "274bb2dca45b: Waiting\n",
      "4105864a46df: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "395e65f0ab42: Waiting\n",
      "382fcf64a9ea: Verifying Checksum\n",
      "382fcf64a9ea: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "382fcf64a9ea: Pull complete\n",
      "395e65f0ab42: Verifying Checksum\n",
      "395e65f0ab42: Download complete\n",
      "9e19ad4dbd7d: Verifying Checksum\n",
      "9e19ad4dbd7d: Download complete\n",
      "957c609522d8: Verifying Checksum\n",
      "957c609522d8: Download complete\n",
      "6a5e2168e631: Verifying Checksum\n",
      "6a5e2168e631: Download complete\n",
      "90cc2e264020: Verifying Checksum\n",
      "90cc2e264020: Download complete\n",
      "be09da654f5c: Verifying Checksum\n",
      "be09da654f5c: Download complete\n",
      "5740bb01bc78: Verifying Checksum\n",
      "5740bb01bc78: Download complete\n",
      "288d40a4f176: Verifying Checksum\n",
      "288d40a4f176: Download complete\n",
      "e2d3eec75c0c: Verifying Checksum\n",
      "e2d3eec75c0c: Download complete\n",
      "3769728eb7d7: Verifying Checksum\n",
      "3769728eb7d7: Download complete\n",
      "211e30f752a4: Verifying Checksum\n",
      "211e30f752a4: Download complete\n",
      "d764c2aa40d3: Verifying Checksum\n",
      "d764c2aa40d3: Download complete\n",
      "4105864a46df: Verifying Checksum\n",
      "4105864a46df: Download complete\n",
      "ae6a5f7af5b1: Verifying Checksum\n",
      "ae6a5f7af5b1: Download complete\n",
      "274bb2dca45b: Verifying Checksum\n",
      "274bb2dca45b: Download complete\n",
      "d764c2aa40d3: Pull complete\n",
      "90cc2e264020: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "395e65f0ab42: Pull complete\n",
      "9e19ad4dbd7d: Pull complete\n",
      "957c609522d8: Pull complete\n",
      "6a5e2168e631: Pull complete\n",
      "5740bb01bc78: Pull complete\n",
      "be09da654f5c: Pull complete\n",
      "288d40a4f176: Pull complete\n",
      "e2d3eec75c0c: Pull complete\n",
      "3769728eb7d7: Pull complete\n",
      "211e30f752a4: Pull complete\n",
      "ae6a5f7af5b1: Pull complete\n",
      "274bb2dca45b: Pull complete\n",
      "4105864a46df: Pull complete\n",
      "Digest: sha256:5290a56a15cebd867722be8bdfd859ef959ffd14f85979a9fbd80c5c2760c3a1\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 0db22ebb67a2\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in ea929536e63e\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 KB 4.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 43.3 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 59.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=d13153b85edc35ef5b6510fd12e1a39248d25df27ee549404b77a96ef13a7cea\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=3038cb621d25ac7fdf23ce0777e99e11c19bd9c64528da17b2bb03b9ee789cce\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=fc6b0f875b11d5c46f80ad5184d2325dfa14e759e4f981c0c8f2f013f284b14d\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, cloudml-hypertune, fire, scikit-learn, pandas\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.1 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "statsmodels 0.13.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.0 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container ea929536e63e\n",
      " ---> cb2376f4efd8\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 41cdc9db6893\n",
      "Removing intermediate container 41cdc9db6893\n",
      " ---> 0fe6be0554cb\n",
      "Step 4/5 : COPY ./train.py /app/\n",
      " ---> 590f8de475cf\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 2899ae6f983d\n",
      "Removing intermediate container 2899ae6f983d\n",
      " ---> d0df2b5f867e\n",
      "Successfully built d0df2b5f867e\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-04-28475851fd68/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-04-28475851fd68/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-04-28475851fd68/trainer_image]\n",
      "e84bbd13ef56: Preparing\n",
      "e41e7c5abdac: Preparing\n",
      "a07f05d6284b: Preparing\n",
      "83a0dd2b9e38: Preparing\n",
      "9638e29d8d24: Preparing\n",
      "b3ab95a574c8: Preparing\n",
      "d1b010151b48: Preparing\n",
      "b80bc089358e: Preparing\n",
      "11bc9b36546a: Preparing\n",
      "43d282ce8d0b: Preparing\n",
      "69fd467ac3a5: Preparing\n",
      "ed4291c31559: Preparing\n",
      "4bf5ae11254c: Preparing\n",
      "0d592bcbe281: Preparing\n",
      "770c4c112e39: Preparing\n",
      "1874048fd290: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "7e897a45d8d8: Preparing\n",
      "42826651fb01: Preparing\n",
      "4236d5cafaa0: Preparing\n",
      "68a85fa9d77e: Preparing\n",
      "4bf5ae11254c: Waiting\n",
      "0d592bcbe281: Waiting\n",
      "770c4c112e39: Waiting\n",
      "1874048fd290: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "7e897a45d8d8: Waiting\n",
      "42826651fb01: Waiting\n",
      "4236d5cafaa0: Waiting\n",
      "68a85fa9d77e: Waiting\n",
      "b3ab95a574c8: Waiting\n",
      "d1b010151b48: Waiting\n",
      "b80bc089358e: Waiting\n",
      "11bc9b36546a: Waiting\n",
      "43d282ce8d0b: Waiting\n",
      "69fd467ac3a5: Waiting\n",
      "ed4291c31559: Waiting\n",
      "9638e29d8d24: Mounted from deeplearning-platform-release/base-cpu\n",
      "83a0dd2b9e38: Mounted from deeplearning-platform-release/base-cpu\n",
      "b3ab95a574c8: Mounted from deeplearning-platform-release/base-cpu\n",
      "d1b010151b48: Mounted from deeplearning-platform-release/base-cpu\n",
      "b80bc089358e: Mounted from deeplearning-platform-release/base-cpu\n",
      "e84bbd13ef56: Pushed\n",
      "11bc9b36546a: Mounted from deeplearning-platform-release/base-cpu\n",
      "e41e7c5abdac: Pushed\n",
      "43d282ce8d0b: Mounted from deeplearning-platform-release/base-cpu\n",
      "69fd467ac3a5: Mounted from deeplearning-platform-release/base-cpu\n",
      "ed4291c31559: Mounted from deeplearning-platform-release/base-cpu\n",
      "4bf5ae11254c: Mounted from deeplearning-platform-release/base-cpu\n",
      "0d592bcbe281: Mounted from deeplearning-platform-release/base-cpu\n",
      "5f70bf18a086: Layer already exists\n",
      "770c4c112e39: Mounted from deeplearning-platform-release/base-cpu\n",
      "1874048fd290: Mounted from deeplearning-platform-release/base-cpu\n",
      "7e897a45d8d8: Mounted from deeplearning-platform-release/base-cpu\n",
      "4236d5cafaa0: Mounted from deeplearning-platform-release/base-cpu\n",
      "42826651fb01: Mounted from deeplearning-platform-release/base-cpu\n",
      "68a85fa9d77e: Layer already exists\n",
      "a07f05d6284b: Pushed\n",
      "latest: digest: sha256:1f8b66865c259fb7ec4c778346afca37b76966d9b7c4c11cc290bdf57425afea size: 4707\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "c024643e-23fd-4a81-acff-47e3b4f75f17  2022-03-30T12:22:07+00:00  1M36S     gs://qwiklabs-gcp-04-28475851fd68_cloudbuild/source/1648642926.912855-fda4dbd88f4643f0877ed1e0991aa950.tgz  gcr.io/qwiklabs-gcp-04-28475851fd68/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an Vertex AI hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the hyperparameter configuration file. \n",
    "Recall that the training code uses `SGDClassifier`. The training application has been designed to accept two hyperparameters that control `SGDClassifier`:\n",
    "- Max iterations\n",
    "- Alpha\n",
    "\n",
    "The file below configures Vertex AI hypertuning to run up to 5 trials in parallel and to choose from two discrete values of `max_iter` and the linear range between `1.0e-4` and `1.0e-1` for `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"forestcover_tuning_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the `config.yaml` file generated below so that the hyperparameter\n",
    "tunning engine try for parameter values\n",
    "* `max_iter` the two values 10 and 20\n",
    "* `alpha` a linear range of values between  1.0e-4 and 1.0e-1\n",
    "\n",
    "Also complete the `gcloud` command to start the hyperparameter tuning job with a max trial count and\n",
    "a max number of parallel trials both of 5 each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: forestcover_tuning_20220330_124310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [6274187731799113728] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai hp-tuning-jobs describe 6274187731799113728 --region=us-central1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "CONFIG_YAML=config.yaml\n",
    "\n",
    "cat <<EOF > $CONFIG_YAML\n",
    "studySpec:\n",
    "  metrics:\n",
    "  - metricId: accuracy\n",
    "    goal: MAXIMIZE\n",
    "  parameters:\n",
    "  - parameterId: max_iter\n",
    "    discreteValueSpec:\n",
    "      values:\n",
    "      - 10\n",
    "      - 20\n",
    "  - parameterId: alpha\n",
    "    doubleValueSpec:\n",
    "      minValue: 1.0e-4\n",
    "      maxValue: 1.0e-1\n",
    "    scaleType: UNIT_LINEAR_SCALE\n",
    "  algorithm: ALGORITHM_UNSPECIFIED # results in Bayesian optimization\n",
    "trialJobSpec:\n",
    "  workerPoolSpecs:  \n",
    "  - machineSpec:\n",
    "      machineType: $MACHINE_TYPE\n",
    "    replicaCount: $REPLICA_COUNT\n",
    "    containerSpec:\n",
    "      imageUri: $IMAGE_URI\n",
    "      args:\n",
    "      - --job_dir=$JOB_DIR\n",
    "      - --training_dataset_path=$TRAINING_FILE_PATH\n",
    "      - --validation_dataset_path=$VALIDATION_FILE_PATH\n",
    "      - --hptune\n",
    "EOF\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=$CONFIG_YAML \\\n",
    "    --max-trial-count=5 \\\n",
    "    --parallel-trial-count=5\n",
    "\n",
    "echo \"JOB_NAME: $JOB_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the Vertex AI Training dashboard and view the progression of the HP tuning job under \"Hyperparameter Tuning Jobs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job completes you can review the results using GCP Console or programmatically using the following functions (note that this code supposes that the metrics that the hyperparameter tuning engine optimizes is maximized): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Complete the body of the function below to retrieve the best trial from the `JOBNAME`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(job_name):\n",
    "    jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "    match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "    tuning_job = match[0] if match else None\n",
    "    return tuning_job.trials if tuning_job else None\n",
    "\n",
    "\n",
    "def get_best_trial(trials):\n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in trials]\n",
    "    best_trial = trials[metrics.index(max(metrics))]\n",
    "    return best_trial\n",
    "\n",
    "\n",
    "def retrieve_best_trial_from_job_name(jobname):\n",
    "    trials = get_trials(jobname)\n",
    "    best_trial = get_best_trial(trials)\n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to wait for the hyperparameter job to complete before being able to retrieve the best job by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = retrieve_best_trial_from_job_name(JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model with the best hyperparameters\n",
    "\n",
    "You can now retrain the model using the best hyperparameters and using combined training and validation splits as a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: \"4\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"alpha\"\n",
      "  value {\n",
      "    number_value: 0.022216002706049832\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"max_iter\"\n",
      "  value {\n",
      "    number_value: 10.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"accuracy\"\n",
      "    value: 0.6710880622319737\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1648644460\n",
      "  nanos: 449031275\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1648644605\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = best_trial.parameters[0].value\n",
    "max_iter = best_trial.parameters[1].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/1081382100180/locations/us-central1/customJobs/435270814913265664] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/1081382100180/locations/us-central1/customJobs/435270814913265664\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/1081382100180/locations/us-central1/customJobs/435270814913265664\n",
      "The model will be exported at: gs://qwiklabs-gcp-04-28475851fd68-kfp-artifact-store/jobs/JOB_VERTEX_20220330_130009\n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"JOB_VERTEX_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-4\"\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "WORKER_POOL_SPEC = f\"\"\"\\\n",
    "machine-type={MACHINE_TYPE},\\\n",
    "replica-count={REPLICA_COUNT},\\\n",
    "container-image-uri={IMAGE_URI}\\\n",
    "\"\"\"\n",
    "\n",
    "ARGS = f\"\"\"\\\n",
    "--job_dir={JOB_DIR},\\\n",
    "--training_dataset_path={TRAINING_FILE_PATH},\\\n",
    "--validation_dataset_path={VALIDATION_FILE_PATH},\\\n",
    "--alpha={alpha},\\\n",
    "--max_iter={max_iter},\\\n",
    "--nohptune\\\n",
    "\"\"\"\n",
    "\n",
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "  --args={ARGS}\n",
    "\n",
    "\n",
    "print(\"The model will be exported at:\", JOB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the training output\n",
    "\n",
    "The training script saved the trained model as the 'model.pkl' in the `JOB_DIR` folder on GCS.\n",
    "\n",
    "**Note:** We need to wait for job triggered by the cell above to complete before running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-04-28475851fd68-kfp-artifact-store/jobs/JOB_VERTEX_20220330_130009/model.pkl\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to Vertex AI Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"forest_cover_classifier_2\"\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest\"\n",
    ")\n",
    "SERVING_MACHINE_TYPE = \"n1-standard-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Upload the trained model using `aiplatform.Model.upload`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/1081382100180/locations/us-central1/models/465722339200335872/operations/295639503974957056\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/1081382100180/locations/us-central1/models/465722339200335872\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/1081382100180/locations/us-central1/models/465722339200335872')\n"
     ]
    }
   ],
   "source": [
    "uploaded_model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=JOB_DIR,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the uploaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Deploy the model using `uploaded_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/1081382100180/locations/us-central1/endpoints/4042095014774308864/operations/1218877427585908736\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/1081382100180/locations/us-central1/endpoints/4042095014774308864\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/1081382100180/locations/us-central1/endpoints/4042095014774308864')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/1081382100180/locations/us-central1/endpoints/4042095014774308864\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/1081382100180/locations/us-central1/endpoints/4042095014774308864/operations/2542935718032834560\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/1081382100180/locations/us-central1/endpoints/4042095014774308864\n"
     ]
    }
   ],
   "source": [
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type=SERVING_MACHINE_TYPE,\n",
    "    accelerator_type=None,\n",
    "    accelerator_count=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve predictions\n",
    "#### Prepare the input file with JSON formated instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Query the deployed model using `endpoint`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[1.0], deployed_model_id='4803045022125522944', explanations=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = [\n",
    "    2841.0,\n",
    "    45.0,\n",
    "    0.0,\n",
    "    644.0,\n",
    "    282.0,\n",
    "    1376.0,\n",
    "    218.0,\n",
    "    237.0,\n",
    "    156.0,\n",
    "    1003.0,\n",
    "    \"Commanche\",\n",
    "    \"C4758\",\n",
    "]\n",
    "\n",
    "endpoint.predict([instance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
